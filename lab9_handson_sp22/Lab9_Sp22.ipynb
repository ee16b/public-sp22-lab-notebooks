{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Lab 9: SVD/PCA Classification for Voice Commands\n",
    "\n",
    "### EECS 16B: Designing Information Devices and Systems II, Spring 2022\n",
    "\n",
    "Written by Nathaniel Mailoa and Emily Naviasky (2016). \n",
    "\n",
    "Updated by Julian Chan (2018), Peter Schafhalter (2019). Vin Ramamurti and Zain Zaidi (Fall 2019)\n",
    "\n",
    "Updated by Kaitlyn Chan, Steven Lu (2021)\n",
    "\n",
    "Updated by Steven Lu, Megan Zeng, Ke Wang (2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Introduction / Lab Note](#intro)\n",
    "* [Part 0: Preparing your Launchpad](#part0)\n",
    "* [Part 1: Setting up your Circuit](#part1)\n",
    "* [Part 2: Data Collection](#part2)\n",
    "* [Part 3: Data Preprocessing](#part3)\n",
    "* [Part 4: PCA via SVD](#part4)\n",
    "* [Part 5: Clustering Data Points](#part5)\n",
    "* [Part 6: Testing your Classifier](#part6)\n",
    "* [Part 7: Tuning for Best Performance](#part7)\n",
    "* [Part 8: Launchpad Implementation of PCA Classify](#part8)\n",
    "* [Appendix: Formatting Vectors for Energia](#appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## <span style=\"color:navy\">Introduction</span>\n",
    "\n",
    "\n",
    "S1XT33N is an obedient little robot that will follow the directions that you tell it. There are four moves that S1XT33N can make: drive straight far, drive straight close, turn left, and turn right. In Lab 7, you implemented the controller to enable S1XT33N to drive straight, and then modified it in Lab 8 to enable S1XT33N to turn. Now, all we have to do is implement voice control, and then put it all together in the last lab for a complete voice-controlled car!\n",
    "\n",
    "That being said, SIXT33N does not speak human languages, and some words, like \"left\" and \"right,\" sound very similar to S1XT33N's ears (a strong single syllable), while other words are easier to distinguish. Our goal in this lab is to find four command words that are easy for SIXT33N to tell apart (consider syllables and intonation). In order to do so, we will develop the PCA classifier that allows S1XT33N to tell the difference between the four commands, and then examine several different words and determine which ones will be easiest to classify by PCA.\n",
    "\n",
    "**Please read the [lab note](https://drive.google.com/file/d/1OXYfQq6EjyaE1lWtA3JkSMWHWi4KIw_o/view?usp=sharing). It explains in detail what you will be doing in each part of the lab.**\n",
    "\n",
    "**Remember to document all design choices you made and explain them in the final lab report.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Lab\n",
    "\n",
    "### Complete the pre-lab assignment before doing the lab. For all students, submit your answers to the Gradescope assignment \"[ALL LAB] Pre-Lab 9: Classification\". Pre-Lab 9 is due on Sunday, April 10 at 11:59pm. No late submissions will be accepted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Classification Procedure\n",
    "Below is an overview of the classification procedure we will be following in this lab.\n",
    "1. Collect recordings of 6 different words. This will form our data set.\n",
    "2. Preprocess our data to align the words\n",
    "2. Split our data into 2 sets: train_data and test_data\n",
    "3. Perform PCA and look at how well it separates the train_data \n",
    "4. Select 4 words to use as your commands.\n",
    "5. Once you have a set of four words that you like, you will compute the means for each of those four words in the PCA basis. We will classify each word according to which mean it is closest in Euclidean distance to. \n",
    "6. To see how well our classifier does on data it has never seen before (this is called generalization in machine learning), we will project test_data onto the same PCA basis as train_data, and find the mean that is closest in Euclidean distance to each data point. \n",
    "7. Make sure you (and your GSI) are satisfied with the classifier's accuracy.\n",
    "\n",
    "The goals of this phase are as follows:\n",
    "- Generate envelope and utilize threshold to get snippets\n",
    "- PCA + Classifier (4 commands)\n",
    "- Check accuracy\n",
    "\n",
    "### Side Note: Datasets in Machine Learning Applications\n",
    "It is common practice, especially in machine learning applications, to split a dataset into a training set and a smaller test set (some common ratios for train:test are 80:20 or 70:30) when trying to make data-driven predictions/decisions. In this lab, we will collect data and split our dataset into 70% training data and 30% test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting your datasets for future lab development\n",
    "\n",
    "- At the end of the lab, please submit your collected `.csv` files for the 4 words you ended up using for your classifier to Gradescope. We will be using student data to help further develop this lab for future semesters. **The Gradescope assignment is \"[Hands-on] SVD/PCA Files.\"**\n",
    "- If you don't want to submit data collected from your voice, feel free to use [Google Translate](https://translate.google.com/) or any other text-to-speech website from your phone as voice alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help Request Link: https://links.eecs16b.org/lab-help-sp22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part0'></a>\n",
    "## <span style=\"color:navy\">Part 0: Preparing your Launchpad</span>\n",
    "\n",
    "**Disconnect the 5V jumper wire that's powering the MSP through the 9V Battery and 9V -> 5V regulator**. As before, make sure that the MSP is not simultaneously being powered by both the computer (via the USB) and the 5V pin. Otherwise, you risk frying your MSP.\n",
    "\n",
    "For the remainder of this lab, the MSP will be powered by only the computer, via the USB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "## <span style=\"color:navy\">Part 1: Verifying your Circuit</span>\n",
    "\n",
    "### Materials\n",
    "- Mic board front-end circuit\n",
    "- Launchpad + USB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Front End Verification\n",
    "\n",
    "1. Hook up your front end circuit. **Make sure you have disconnected the 5V pin on the Launchpad**.\n",
    "2. Without powering your circuit from the power supply yet, connect your circuit to the Launchpad:\n",
    "    - **P6.0 to the microphone front end circuit output (output of non-inverting amplifier for low-pass filter).**\n",
    "    - GND pin to the ground rail of the breadboard.\n",
    "    - You can keep your Launchpad plugged in via USB as long as **YOUR 5V JUMPER IS DISCONNECTED**.\n",
    "3. Use the bench power supply to provide 9V to the 9V->5V and 9V->3.3V voltage regulators. You won't be using the motor circuits for this lab, so you can leave that part of the circuit unpowered. \n",
    "4. Set the current limit to 0.1A.\n",
    "5.  **Use the oscilloscope to probe the output of the microphone circuit (output of non-inverting amplifier for low-pass filter).** Make sure the waveform averages to 1.65V (halfway between 0V and 3.3V) and the peak-to-peak is large enough.\n",
    "    - Talk at a comfortable distance away from the microphone; you should see the signal change to reflect the sounds you just made. If you are close enough or loud enough, you should be able to get the peak-to-peak of your signal all the way up to around 2.5V without seeing railing behavior.\n",
    "    - If not, then you may need to retune the mic board, and/or increase the gain of the non-inverting amplifier for the low-pass filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "## <span style=\"color:navy\">Part 2: Data Collection</span>\n",
    "\n",
    "### Materials\n",
    "- Microphone front-end circuit\n",
    "- Launchpad + USB\n",
    "\n",
    "We will begin by collecting our data that we will be performing PCA analysis on later in the lab. To do so, we need to think of a set of words in which each word is ideally easily distinguished from each other, so that when we go to classify the live voice command in the last part of the lab, we don't end up misclassifying it and executing the wrong drive command.\n",
    "\n",
    "When humans distinguish words, they listen for temporal and frequency differences to determine what is being said. However, SIXT33N does not have the memory or the processing power to distinguish words nearly as well as our human brains, so we will have to choose much simpler features for SIXT33N to look at (syllables, intonation, magnitude).\n",
    "\n",
    "When you think of speech signals, you might notice that the shape of the speech wave is a very distinctive part of each word. Taking just the shape of the magnitude of a signal is called enveloping, exemplified in the image below. So, we want to do some filtering to retrieve the envelope of the audio signal. We train the PCA off of just this envelope and build a classifier to classify new data points.\n",
    "\n",
    "<center>\n",
    "<img width=\"400px\" src=\"images/proj-envelope.png\">\n",
    "</center>\n",
    "\n",
    "<b>Keeping in mind that the words that look most different have different shapes (or different amplitudes varied over time), brainstorm at least six words that you think will sort well. Consider syllables, intonation, and length of the word.</b>\n",
    "\n",
    "**<span style=\"color:red\">What words are you going to try? Why?</span>** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Now we will record 45-50 audio samples for each of your 6 chosen words. We recommend each partner records 3 words each, to maximize variance between our recorded signals. Again, think about how to make the words distinct from others (syllable count, intonation). For each word, make sure to note who said it and how it was said (like through a video/audio recording on your phone) so that it is easier to reproduce later for live classification.\n",
    "\n",
    "Make sure your breadboard is powered and you see an audio signal at the microphone front end circuit's output! **Also make sure P6.0 on Launchpad is connected to the microphone front end circuit output (output of non-inverting amplifier for low-pass filter)!**\n",
    "\n",
    "**For each chosen word, do the following:**\n",
    "1. Upload the sketch **`collect-data-envelope.ino`** to your Launchpad.\n",
    "    - This sketch records 2 seconds of audio sampled every 0.35ms at a time and sends it to your computer.\n",
    "2. Run **`python collect-data-envelope.py YOUR_WORD.csv`**.\n",
    "    - Download the collect-data-envelope folder to your local computer. Press Shift + right click within the file explorer to view the option to \"Open PowerShell window here\".\n",
    "    - Open the PowerShell terminal and navigate to the directory with the `collect-data-envelope.py` script and then run the above command.\n",
    "    - Make sure the Serial Monitor/Plotter in Energia is closed before running the script!\n",
    "    - This program will capture audio data collected by the Launchpad and write it to `YOUR_WORD.csv`. You should see a console output in your PowerShell terminal after each sample is recorded. \n",
    "    - Choose your words carefully! Think about the PCA algorithm and what characteristics of your word might affect its output.\n",
    "3. **When the red light goes on, say the word you want to record.**\n",
    "    - **Pronounce the word consistently and always speak around the same time relative to when the red light turns on.** This will help you collect data that is less \"noisy\" which will result in better classification. \n",
    "    - The Launchpad is recording only when the light is on, so finish the word before the red LED turns off.\n",
    "    - \"Good\" audio data has a high signal to noise ratio. Recording words while far away from the microphone may cause your intended word to blend in with background noise. However \"oversaturation\" of the audio signal (speaking too loudly and/or too closely into the mic) will also distort the signal. You can probe the microphone output using the oscilloscope to test for oversaturation/undersaturation.\n",
    "    - **We recommend that after taking 3 or 4 recordings for the first time, stop the program (e.g. by pressing Ctrl + C in the command prompt) and check `YOUR_WORD.csv` and make sure that it looks like a sound wave as opposed to being full of super low values. *It might help to graph the data as a line plot in Excel.*** You don't want to record 50 times only to find that your mic board wasn't working.\n",
    "4. Once you've recorded 45-50 good audio samples of the word (excluding any recordings where you missed the timing, like at the beginning), stop the Python program (e.g. by pressing Ctrl + C in the command prompt).\n",
    "5. Go into the .csv file and delete outlier samples such that you are left with **exactly 40 audio recordings of the word**. Outliers are often near the beginning and end of the .csv file when you may not be speaking. The best way to help you identity outlier samples is to plot the data in Excel, and to identify specific recordings, you can plot individual rows. Plot all your samples using a line plot. **Don't spend too much time with this,** our enveloping function in Part 3 also helps with some of the outliers!\n",
    "6. If you are working on the lab on DataHub, you will need to upload your `.csv` files to DataHub into the `PCA_data` folder so the Jupyter notebook can access them.\n",
    "\n",
    "\n",
    "### Before moving on, please note that:\n",
    "\n",
    "You may realize in the next section that one or two of your words are not sorting quite as well as you would like. Don't be afraid to come back to this section and try collecting different words based on what you have learned makes a word sortable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3'></a>\n",
    "## <span style=\"color:navy\">Part 3: Data Preprocessing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the recorded data for PCA, we must first process the data. If we don't, PCA will not work well because the different recordings of the same word can look wildly different, depending on factors like when you started saying the word and how quickly you said it (assuming you are not a robot that can repeat the word 50 times in the exact same way). \n",
    "\n",
    "Using the Launchpad, we have already implemented the first step of processing the audio recording: enveloping. It is not necessary for you to understand the enveloping function well enough to implement it (since we have already done it for you), but just in case you are curious, the enveloping function is described in the following pseudocode:\n",
    "\n",
    "<code><b>Enveloping function</b>\n",
    "Divide the whole signal to a block of 16 samples\n",
    "For each chunk:\n",
    "    Find the mean of the chunk\n",
    "    Subtract each sample by the mean\n",
    "    Find the sum of the absolute value of each sample\n",
    "</code>\n",
    "\n",
    "### 3.1 Load Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import csv\n",
    "!pip install optuna\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings('ignore')\n",
    "import utils\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "cm = ['blue', 'red', 'green', 'orange', 'black', 'purple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Fill in the six words you recorded\n",
    "all_words_arr = ['', '', '', '', '', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by splittig our data into two sets. As we stated in the introduction, we will be splitting our data into the training set and test set via a 70%/30% split. Run the code below to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv\n",
    "train_test_split_ratio = 0.7\n",
    "train_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "# Build the dictionary of train and test samples.\n",
    "for i in range(len(all_words_arr)):\n",
    "    word_raw = utils.read_csv(\"PCA_data/{}.csv\".format(all_words_arr[i]))\n",
    "    word_raw_train, word_raw_test = utils.train_test_split(word_raw, train_test_split_ratio)\n",
    "    train_dict[all_words_arr[i]] = word_raw_train\n",
    "    test_dict[all_words_arr[i]] = word_raw_test\n",
    "\n",
    "# Count the minimum number of samples you took across the six recorded words. These variables might be useful for you!\n",
    "num_samples_train = min(list(map(lambda x : np.shape(x)[0], train_dict.values())))\n",
    "num_samples_test = min(list(map(lambda x : np.shape(x)[0], test_dict.values())))\n",
    "\n",
    "# Crop the number of samples for each word to the minimum number so all words have the same number of samples.\n",
    "for key, raw_word in train_dict.items():\n",
    "    train_dict[key] = raw_word[:num_samples_train,:]\n",
    "\n",
    "for key, raw_word in test_dict.items():\n",
    "    test_dict[key] = raw_word[:num_samples_test,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot your training data and get a feel for how it looks enveloped. Spend a little time looking at the data you just collected in the Python plots below.\n",
    "\n",
    "**<span style=\"color:red\">Important: It's okay if the data isn't aligned. The code in the next part will align the data.</span>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all training samples\n",
    "word_number = 0\n",
    "selected_words_arr = all_words_arr\n",
    "for word_raw_train in train_dict.values():\n",
    "    plt.plot(word_raw_train.T)\n",
    "    plt.title('Training sample for \"{}\"'.format(selected_words_arr[word_number]))\n",
    "    word_number += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Align Audio Recordings\n",
    "\n",
    "As you can see above, the speech is only a small part of the 2 second window, and each sample starts at different times. PCA is not good at interpreting delay, so we need to somehow start in the same place each time and capture a smaller segment of the 2 second sample where the speech is present. To do this, we will use a thresholding algorithm.\n",
    "\n",
    "First, we define a **`threshold`** relative to the maximum value of the data. We say that any signal that crosses the threshold is the start of a speech command. In order to not lose the first couple samples of the speech command, we say that the command starts **`pre_length`** samples before the threshold is crossed. We then take a window of the data that is **`length`** long, and try to capture the entire sound of the command in that window.\n",
    "\n",
    "<b>The parameters `length`, `pre_length` and `threshold`</b> are defined with default values in the cells below. The default length value corresponds to a reasonable length for vectors we can use, because the Launchpad has limited storage and memory and cannot store too much.\n",
    "\n",
    "Finding the optimal set of parameters is an arduous task that we would rather not accomplish manually, so we will be leaving that to a software algorithm to optimize for us. However, this requires us to implement the entire classifier in code first, as we need to give the algorithm something to optimize around (the classification rate). As such, we will use the default values given in the third code block below to preprocess our data, and use that to help guide us through the process of implementing our classifier. After this, you will have a visual reference for how things look after each step in the classifier, before we move on to optimizing these parameters algorithmically. You should see the results and how much of your command you captured in the plots generated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snippets(data, length, pre_length, threshold):\n",
    "    \"\"\"Attempts to align audio samples in data.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): Matrix where each row corresponds to a recording's audio samples.\n",
    "        length (int): The length of each aligned audio snippet.\n",
    "        pre_length (int): The number of samples to include before the threshold is first crossed.\n",
    "        threshold (float): Used to find the start of the speech command. The speech command begins where the\n",
    "            magnitude of the audio sample is greater than (threshold * max(samples)).\n",
    "    \n",
    "    Returns:\n",
    "        Matrix of aligned recordings.\n",
    "    \"\"\"\n",
    "    assert isinstance(data, np.ndarray) and len(data.shape) == 2, \"'data' must be a 2D matrix\"\n",
    "    assert isinstance(length, int) and length > 0, \"'length' of snippet must be an integer greater than 0\"\n",
    "    assert 0 <= threshold <= 1, \"'threshold' must be between 0 and 1\"\n",
    "    snippets = []\n",
    "\n",
    "    # Iterate over the rows in data\n",
    "    for recording in data:\n",
    "        # Find the threshold\n",
    "        recording_threshold = threshold * np.max(recording)\n",
    "\n",
    "        # Figure out when interesting snippet starts\n",
    "        i = pre_length\n",
    "        while recording[i] < recording_threshold:\n",
    "            i += 1\n",
    "            \n",
    "        snippet_start = min(i - pre_length, len(recording) - length)\n",
    "        snippet = recording[snippet_start:snippet_start + length]\n",
    "\n",
    "        # Normalization\n",
    "        snippet = snippet / np.sum(snippet)\n",
    "        \n",
    "        snippets.append(snippet)\n",
    "\n",
    "    return np.vstack(snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for get_snippets\n",
    "def process_data(dict_raw, length, pre_length, threshold, plot=True):\n",
    "    \"\"\"\n",
    "    Process the raw data given parameters and return it.\n",
    "    \n",
    "    Args:\n",
    "        dict_raw (np.ndarray): Raw data collected.\n",
    "        data (np.ndarray): Matrix where each row corresponds to a recording's audio samples.\n",
    "        length (int): The length of each aligned audio snippet.\n",
    "        pre_length (int): The number of samples to include before the threshold is first crossed.\n",
    "        threshold (float): Used to find the start of the speech command. The speech command begins where the\n",
    "            magnitude of the audio sample is greater than (threshold * max(samples)).\n",
    "        plot (boolean): Plot the dataset if true.\n",
    "            \n",
    "    Returns:\n",
    "        Processed data dictionary.\n",
    "    \"\"\"\n",
    "    processed_dict = {}\n",
    "    word_number = 0\n",
    "    for key, word_raw in dict_raw.items():\n",
    "        word_processed = get_snippets(word_raw, length, pre_length, threshold)\n",
    "        processed_dict[key] = word_processed\n",
    "        if plot:\n",
    "            plt.plot(word_processed.T)\n",
    "            plt.title('Samples for \"{}\"'.format(selected_words_arr[word_number]))\n",
    "            word_number += 1\n",
    "            plt.show()\n",
    "            \n",
    "    return processed_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 80 # Default: 80\n",
    "pre_length = 5 # Default: 5\n",
    "threshold = 0.5 # Default: 0.5\n",
    "\n",
    "processed_train_dict = process_data(train_dict, length, pre_length, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see a more organized set of samples for each word. Can you tell which word is which just by the envelope? If an of your words looks nearly identical to another and you can't tell the words apart, then PCA will likely have a difficult time as well, so you may want to consider re-recording."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4'></a>\n",
    "## <span style=\"color:navy\">Part 4: PCA via SVD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 SVD/PCA Resources\n",
    "- http://www.ams.org/publicoutreach/feature-column/fcarc-svd\n",
    "- https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n",
    "- https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generate and Preprocess PCA Matrix\n",
    "\n",
    "Now that we have our data in a nice format, we can build the PCA input matrix from that data by **stacking all the data vertically**.\n",
    "\n",
    "**Sanity check:** What should be the dimensions of processed_A? Feel free to use np.shape() if you aren't sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_A = np.vstack(list(processed_train_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of PCA is to zero-mean your data as `demeaned_A`. Centering the data can be helpful to obtain principal components that are representative of the shape of the variations in the data. Please note that you want to **get the mean of each feature** (***what are the features?***). The function [`np.mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html) might be helpful here, along with specifying the axis parameter.\n",
    "\n",
    "**Sanity check:** Does the shape of `mean_vec` make sense given what we averaged across?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-mean the matrix A\n",
    "# YOUR CODE HERE\n",
    "mean_vec = ...\n",
    "demeaned_A = ...\n",
    "print(processed_A.shape)\n",
    "print(mean_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Principal Component Analysis\n",
    "\n",
    "Next, take the SVD of your demeaned data. Don't worry about the \"Line 1 for parameter tuning\" for now, we will come back to that later when we optimize length, prelength, and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the SVD of matrix demeaned_A (np.linalg.svd)\n",
    "# YOUR CODE HERE #\n",
    "U, S, Vt = ...          # Line 1 for parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at your sigma values. They should show you very clearly how many principal components you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out the sigma values (Hint: Use plt.stem for a stem plot)\n",
    "# YOUR CODE HERE #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">How many principal components do you need? Given that you are sorting 6 words, what is the number you expect to need?</span>** \n",
    "\n",
    "There is no correct answer here. We can pick as many principal components onto which we project our data to get the \"best\" separation (most variance), but at some point, the cost-benefit isn't worth selecting an extra basis vector. For example, in our project, we are loading these basis vectors onto the [MSP430 Launchpad](http://www.ti.com/tool/MSP-EXP430F5529LP), and we can only store at most 3 principal components before we run into memory issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Choosing a Basis using Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `new_basis` argument to be a basis of the first 3 principal components. (Hint: Of the three outputs from the SVD function call, which one will contain the principal components onto which we want to project our data points? Do we need to transpose it? **The lab note will help!**)\n",
    "\n",
    "When you plot `new_basis` you should see a number of line plots equal to the number of principal components you've chosen (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the principal component(s)\n",
    "# YOUR CODE HERE\n",
    "new_basis = ...        # This should be the basis containing your principal components. Line 2 for parameter tuning\n",
    "plt.plot(new_basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now project the data in the matrix A onto the new basis and plot it. For three principal components, in addition to the 3D plot, we also provided 2D plots which correspond to the top and side views of the 3D plot. Do you see clustering? Do you think you can separate the data easily? Don't worry too much if it doesn't look perfect, as we haven't optimized our preprocessing parameters yet. As long as you see clustering, it's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Project the data onto the new basis\n",
    "# YOUR CODE HERE. Hint: np.dot() may help, as well as printing the dimensions.\n",
    "proj = ...\n",
    "\n",
    "if new_basis.shape[1] == 3:\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(len(all_words_arr)):\n",
    "        Axes3D.scatter(ax, *proj[i*num_samples_train:num_samples_train*(i+1)].T, c=cm[i], marker = 'o', s=20)\n",
    "    plt.legend(all_words_arr,loc='center left', bbox_to_anchor=(1.07, 0.5))\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        axs[0].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], c=cm[i], edgecolor='none')\n",
    "        axs[1].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "        axs[2].scatter(proj[i*num_samples_train:num_samples_train*(i+1),1], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "    axs[0].set_title(\"View 1\")\n",
    "    axs[1].set_title(\"View 2\")\n",
    "    axs[2].set_title(\"View 3\")\n",
    "    plt.legend(all_words_arr,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "elif new_basis.shape[1] == 2:\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        plt.scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], edgecolor='none')\n",
    "\n",
    "    plt.legend(all_words_arr,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your data might look noisy, and might not classify perfectly. That is completely okay, we are just looking for good enough. Like many AI applications, this is noisy data that we are classifying so some error in classification is okay. The important part is that you see strong clustering of your words. \n",
    "\n",
    "If you don't see clustering, try to think about why this might be the case. Things you might want to ask yourself:\n",
    "- How does PCA create the clusters? \n",
    "- What characteristics of your waveform will PCA favor when clustering? \n",
    "- How can you choose your words such that it maximizes the distinction between your different classes?\n",
    "\n",
    "Once you think you have decent clustering, you can move on to getting your code to automate classification and you will make up for some of the error there, too. **Choose 4 out of the 6 words that form the most distinct clusters. You will be using these four words for the rest of this lab.** Again, this will also be optimized later by our algorithm, so don't worry too much about choosing the perfect words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "selected_words_arr = ['', '', '', '']\n",
    "\n",
    "# Select data\n",
    "selected_train_dict = {k: train_dict[k] for k in selected_words_arr}\n",
    "selected_processed_train_dict = {k: processed_train_dict[k] for k in selected_words_arr}\n",
    "selected_test_dict = {k: test_dict[k] for k in selected_words_arr}\n",
    "\n",
    "num_samples_train = min(list(map(lambda x : np.shape(x)[0], selected_train_dict.values())))\n",
    "num_samples_test = min(list(map(lambda x : np.shape(x)[0], selected_test_dict.values())))\n",
    "\n",
    "# Reconstruct data based on 4 chosen words.\n",
    "processed_A = np.vstack(list(selected_processed_train_dict.values()))\n",
    "mean_vec = np.mean(processed_A, axis=0)\n",
    "demeaned_A = processed_A - mean_vec\n",
    "proj = demeaned_A.dot(new_basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part5'></a>\n",
    "## <span style=\"color:navy\">Part 5: Clustering Data Points</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement `find_centroids` which finds the center of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centroids(clustered_data, num_samples_train):\n",
    "    \"\"\"Find the center of each cluster by taking the mean of all points in a cluster.\n",
    "    It may be helpful to recall how you constructed the data matrix (e.g. which rows correspond to which word)\n",
    "    \n",
    "    Parameters:\n",
    "        clustered_data: the data already projected onto the new basis\n",
    "        num_samples_train: the number of samples trained\n",
    "        \n",
    "    Returns: \n",
    "        The centroids of the clusters\n",
    "    \"\"\"\n",
    "    centroids = []\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: the variable num_samples_train may help you splice into your clustered_data, as well as np.mean()\n",
    "    # Feel free to ignore the skeleton code if you wish to write it your way.\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code below to find the centroids of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the centroids of each cluster\n",
    "# YOUR CODE HERE: hint: call find_centroids()\n",
    "centroids = ...          # Line 3 for parameter tuning\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to plot your centroids along with your projected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_list = np.vstack(centroids)\n",
    "colors = cm[:(len(centroids))]\n",
    "\n",
    "for i, centroid in enumerate(centroid_list):\n",
    "    print('Centroid {} is at: {}'.format(i, str(centroid)))\n",
    "\n",
    "if new_basis.shape[1] == 3:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        Axes3D.scatter(ax, *proj[i*num_samples_train:num_samples_train*(i+1)].T, c=cm[i], marker = 'o', s=20)\n",
    "    plt.legend(selected_words_arr, loc='center left', bbox_to_anchor=(1.07, 0.5))\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        Axes3D.scatter(ax, *np.array([centroids[i]]).T, c=cm[i], marker = '*', s=300)\n",
    "    plt.title(\"Training Data\")\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        axs[0].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], c=cm[i], edgecolor='none')\n",
    "        axs[1].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "        axs[2].scatter(proj[i*num_samples_train:num_samples_train*(i+1),1], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "    axs[0].set_title(\"View 1\")\n",
    "    axs[1].set_title(\"View 2\")\n",
    "    axs[2].set_title(\"View 3\")\n",
    "    plt.legend(selected_words_arr, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    axs[0].scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    axs[1].scatter(centroid_list[:,0], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    axs[2].scatter(centroid_list[:,1], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    plt.show()\n",
    "\n",
    "elif new_basis.shape[1] == 2:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        plt.scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], c=colors[i], edgecolor='none')\n",
    "\n",
    "    plt.scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    plt.legend(selected_words_arr,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(\"Training Data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part6'></a>\n",
    "## <span style=\"color:navy\">Part 6: Testing your Classifier</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We now have the means (centroid) for each word. Now let's see how well our test data performs. Recall that we will classify each data point according to the centroid that it is closest in Euclidean distance to. \n",
    "\n",
    "Before we perform classification, we need to do the same preprocessing to the test data that we did to the training data (enveloping, demeaning, projecting onto the PCA basis). You have already written most of the code for this part. However, note the difference in variable names as we are now working with test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at what our raw test data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all test samples\n",
    "word_number = 0\n",
    "for word_raw_test in selected_test_dict.values():\n",
    "    plt.plot(word_raw_test.T)\n",
    "    plt.title('Test sample for \"{}\"'.format(selected_words_arr[word_number]))\n",
    "    word_number += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform enveloping and trimming of our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_dict = process_data(selected_test_dict, length, pre_length, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the PCA matrix by stacking all the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_processed_test_dict = {k: processed_test_dict[k] for k in selected_words_arr}\n",
    "\n",
    "processed_A_test = np.vstack(list(selected_processed_test_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we will do something slightly different.**\n",
    "\n",
    "Previously, you projected data onto your PCA basis with $ (x - \\bar{x})P $, where $\\bar{x}$ is the mean vector, x is a single row of `processed_A`, and P is `new_basis`. \n",
    "\n",
    "We can rewrite this operation as \n",
    "\n",
    "$$ (x - \\bar{x})P = xP - \\bar{x}P = xP - \\bar{x}_{\\text{proj}} $$ \n",
    "$$ \\bar{x}_{\\text{proj}} = \\bar{x}P $$\n",
    "\n",
    "Why might we want to do this? We'll later perform these operations on our car. Our Launchpads have limited memory, so we want to store as little as possible. Instead of storing a length $n$ vector $\\bar{x}$, we can precompute $ \\bar{x}_{\\text{proj}} $ (length 3) and store that instead!\n",
    "\n",
    "Compute $ \\bar{x}_{\\text{proj}} $ using the **same mean vector** as the one computed with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "projected_mean_vec = ...                   # Line 4 for parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the test data onto the **same PCA basis** as the one computed with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "projected_A_test = ...                     # Line 5 for parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-mean the projected test data using the **`projected_mean_vec`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "proj = ...                                 # Line 6 for parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the projections to see how well your test data clusters in this new basis. This will give you an idea of how well your test data will classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_basis.shape[1] == 3:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        Axes3D.scatter(ax, *proj[i*num_samples_test:num_samples_test*(i+1)].T, c=cm[i], marker = 'o', s=20)\n",
    "    plt.legend(selected_words_arr,loc='center left', bbox_to_anchor=(1.07, 0.5))\n",
    "    plt.title(\"Test Data\")\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        Axes3D.scatter(ax, *np.array([centroids[i]]).T, c=cm[i], marker = '*', s=300)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        axs[0].scatter(proj[i*num_samples_test:num_samples_test*(i+1),0], proj[i*num_samples_test:num_samples_test*(i+1),1], c=cm[i], edgecolor='none')\n",
    "        axs[1].scatter(proj[i*num_samples_test:num_samples_test*(i+1),0], proj[i*num_samples_test:num_samples_test*(i+1),2], c=cm[i], edgecolor='none')\n",
    "        axs[2].scatter(proj[i*num_samples_test:num_samples_test*(i+1),1], proj[i*num_samples_test:num_samples_test*(i+1),2], c=cm[i], edgecolor='none')\n",
    "    axs[0].set_title(\"View 1\")\n",
    "    axs[1].set_title(\"View 2\")\n",
    "    axs[2].set_title(\"View 3\")\n",
    "    plt.legend(selected_words_arr, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    axs[0].scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    axs[1].scatter(centroid_list[:,0], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    axs[2].scatter(centroid_list[:,1], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    fig.show()\n",
    "\n",
    "elif new_basis.shape[1] == 2:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        plt.scatter(proj[i*num_samples_test:num_samples_test*(i+1),0], proj[i*num_samples_test:num_samples_test*(i+1),1], c=colors[i], edgecolor='none')\n",
    "\n",
    "    plt.scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    plt.legend(selected_words_arr,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(\"Test Data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some idea of how our test data looks in our PCA basis, let's see how our data actually performs. Implement the classify function that takes in a data point (AFTER enveloping is applied) and returns which word number it belongs to depending on which centroid the data point is closest in Euclidean distance to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data_point, new_basis, projected_mean_vec, centroids):\n",
    "    \"\"\"Classifies a new voice recording into a word.\n",
    "    \n",
    "    Args:\n",
    "        data_point: new data point vector before demeaning and projection\n",
    "        new_basis: the new processed basis to project on\n",
    "        projected_mean_vec: the same projected_mean_vec as before\n",
    "    Returns:\n",
    "        Word number (should be in {1, 2, 3, 4} -> you might need to offset your indexing!)\n",
    "    Hint:\n",
    "        Remember to use 'projected_mean_vec'!\n",
    "        Np.argmin(), and np.linalg.norm() may also help!\n",
    "    \"\"\"\n",
    "    # TODO: classify the demeaned data point by comparing its distance to the centroids\n",
    "    # YOUR CODE HERE\n",
    "    projected_data_point = ...\n",
    "    demeaned = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out the classification function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the classification function\n",
    "print(classify(processed_A_test[0,:], new_basis, projected_mean_vec, centroids)) # Modify the row index of processed_A_test to use other vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our goal is 80% accuracy for each word.** Apply the `classify` function to each sample and compute the accuracy for each word. Don't worry if you don't meet the goal right now; we are about to optimize our preprocessing parameters in the next part of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to classify the whole A matrix\n",
    "correct_counts = np.zeros(4)\n",
    "\n",
    "for (row_num, data) in enumerate(processed_A_test):\n",
    "    word_num = row_num // num_samples_test + 1\n",
    "    if classify(data, new_basis, projected_mean_vec, centroids) == word_num:\n",
    "        correct_counts[word_num - 1] += 1\n",
    "        \n",
    "for i in range(len(correct_counts)):\n",
    "    print(\"Percent correct of word {} = {}%\".format(i + 1, 100 * correct_counts[i] / num_samples_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part7'></a>\n",
    "## <span style=\"color:navy\">Part 7: Tuning for Best Performance</span>\n",
    "In this part, we will optimize our preprocessing parameters to achieve the best classification rate. **You do not need to understand any of this code for the optimizer**. This code comes courtesy of Andris Huang, a student in EECS 16B Fall 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function the optimizer will use. Copy the lines marked with a comment \"Line # for parameter tuning\" from up above into the corresponding lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(length, pre_length, threshold, selected_words_arr):\n",
    "    processed_train_dict = process_data(train_dict, length, pre_length, threshold, plot=False)\n",
    "    processed_A = np.vstack(list(processed_train_dict.values()))\n",
    "    mean_vec = np.mean(processed_A, axis=0)\n",
    "    demeaned_A = processed_A - mean_vec\n",
    "    \n",
    "    U, S, Vt = ... # Copy Line 1 Here\n",
    "    new_basis = ... # Copy Line 2 Here\n",
    "    \n",
    "    selected_train_dict = {k: train_dict[k] for k in selected_words_arr}\n",
    "    selected_processed_train_dict = {k: processed_train_dict[k] for k in selected_words_arr}\n",
    "    selected_test_dict = {k: test_dict[k] for k in selected_words_arr}\n",
    "    processed_A = np.vstack(list(selected_processed_train_dict.values()))\n",
    "    mean_vec = np.mean(processed_A, axis=0)\n",
    "    demeaned_A = processed_A - mean_vec\n",
    "    proj = demeaned_A.dot(new_basis)\n",
    "    \n",
    "    num_samples_train = min(list(map(lambda x : np.shape(x)[0], selected_train_dict.values())))\n",
    "    num_samples_test = min(list(map(lambda x : np.shape(x)[0], selected_test_dict.values())))\n",
    "\n",
    "    centroids = ... # Copy Line 3 Here\n",
    "    \n",
    "    processed_test_dict = process_data(selected_test_dict, length, pre_length, threshold, plot=False)\n",
    "    selected_processed_test_dict = {k: processed_test_dict[k] for k in selected_words_arr}\n",
    "    processed_A_test = np.vstack(list(selected_processed_test_dict.values()))\n",
    "    \n",
    "    projected_mean_vec = ... # Copy Line 4 Here\n",
    "    projected_A_test = ... # Copy Line 5 Here\n",
    "    proj = ... # Copy Line 6 Here\n",
    "    \n",
    "    return processed_A_test, new_basis, projected_mean_vec, centroids, num_samples_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the objective the optimizer tries to optimize. The optimizer uses all_words_arr with ranges for length (default 70 to 90), pre_length (default 0 to 9), and threshold (default 0 to 0.99) and considers different combinations of words with different parameter values within these ranges. Note that this default range may not be what is best for your set of words; it may not capture the entire length of your words or it may capture too much silence around them instead. You may need to adjust this range with your specific words in mind, though you should not go above 100 in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines an objective that we try to optimize. In this lab, it's the accuracy for classification. \n",
    "    Simply run this block.\n",
    "    \n",
    "    Args:\n",
    "        trial (object): a specific trial with given combinations of parameters\n",
    "        \n",
    "    Returns:\n",
    "        -1 if the given combination of param is not compatible with the model;\n",
    "        The number of past test cases if fewer than 4 cases are passed. Thus, returns value in [0, 3];\n",
    "        The average percent correct of word if all 4 test cases are passed. The % sign is omitted.\n",
    "    \"\"\"\n",
    "    length = trial.suggest_int('length', 70, 90) # Default: 80               # Suggest range for length\n",
    "    pre_length = trial.suggest_int('pre_length', 0, 9) # Default: 5          # Suggest range for pre_length\n",
    "    threshold = trial.suggest_float('threshold', 0., 0.99) # Default: 0.5    # Suggest range for threshold\n",
    "    selected_words_arr = trial.suggest_categorical('selected_words_arr', \n",
    "        list(itertools.combinations(all_words_arr, 4)))                      # Suggest word combinations\n",
    "\n",
    "    try:\n",
    "        data = load_data(length, pre_length, threshold, selected_words_arr)\n",
    "        processed_A_test, new_basis, projected_mean_vec, centroids, num_samples_test = data\n",
    "    except:\n",
    "        return -1 # the given combination of param is not compatible with the model\n",
    "    \n",
    "    correct_counts = np.zeros(4)\n",
    "    for (row_num, data) in enumerate(processed_A_test):\n",
    "        word_num = row_num // num_samples_test + 1\n",
    "        if classify(data, new_basis, projected_mean_vec, centroids) == word_num:\n",
    "            correct_counts[word_num - 1] += 1\n",
    "                \n",
    "    num_pass = 0\n",
    "    total_auc = 0\n",
    "    for i in range(len(correct_counts)):\n",
    "        auc = 100 * correct_counts[i] / num_samples_test\n",
    "        total_auc += auc\n",
    "        if auc >= 80:\n",
    "            num_pass += 1\n",
    "    if num_pass < 4:\n",
    "        return num_pass\n",
    "    return total_auc / len(correct_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the optimizer. You do not need to change any of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = utils.create_study()\n",
    "study.optimize(objective, n_trials=50, timeout=60) # Can change the number of trials and timeout (in seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best stats\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our optimized words list and preprocessing parameters, let's try our classification again and see if it improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_opt = trial.params['length']\n",
    "pre_length_opt = trial.params['pre_length']\n",
    "threshold_opt = trial.params['threshold']\n",
    "selected_words_arr_opt = trial.params['selected_words_arr']\n",
    "processed_A_test_opt, new_basis_opt, projected_mean_vec_opt, centroids_opt = load_data(length, pre_length, threshold, selected_words_arr)[:4]\n",
    "\n",
    "correct_counts = np.zeros(4)\n",
    "\n",
    "for (row_num, data) in enumerate(processed_A_test_opt):\n",
    "    word_num = row_num // num_samples_test + 1\n",
    "    if classify(data, new_basis_opt, projected_mean_vec_opt, centroids_opt) == word_num:\n",
    "        correct_counts[word_num - 1] += 1\n",
    "        \n",
    "for i in range(len(correct_counts)):\n",
    "    print(\"Percent correct of word {} = {}%\".format(i + 1, 100 * correct_counts[i] / num_samples_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to meet the 80% classification accuracy we require! If not, you may need to re-record some of your words or adjust some parameter ranges for the optimizer. Otherwise, since the optimizer is not perfect, if your original parameters worked better, feel free to use those. Make sure to note down the values of `length`, `pre_length` and `threshold` that you choose - <b>you will need to add them to the Launchpad sketch later!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part8'></a>\n",
    "## <span style=\"color:navy\">Part 8: Launchpad Implementation of PCA Classify</span>\n",
    "\n",
    "### Materials\n",
    "- Microphone front-end circuit\n",
    "- Launchpad + USB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our completed classifier, we are now ready to classify live commands using our Launchpad. This section will walk you through implementing your classification algorithm on the Launchpad. You will need to transfer the preprocessing parameters, PCA vectors, projected mean vector, and centroids all into the Launchpad. **You will be copying and pasting the code from the Appendix of the SVD/PCA notebook into `classify.ino.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task will be to implement your <b>data processing</b> and <b>classification</b> (just the projection, not the PCA) in the Launchpad sketch <b>`classify.ino`</b>. Since Energia does not have as many built-in functions as Python, you will have to write out the functions yourself. For example, a dot product should be written as:\n",
    "\n",
    "```C\n",
    "float dot_product_result = 0;\n",
    "for (i = 0; i < LENGTH; i++) {\n",
    "    dot_product_result += vector1[i] * vector2[i];\n",
    "}\n",
    "```\n",
    "where `dot_product_result` is the result of the dot product, and `vector1` and `vector2` are the two vectors you're taking the dot product of.\n",
    "\n",
    "NOTE: The coding language is a derivative of C/C++, so you need to follow C syntax! i.e. declaring variables before using them, using `{` and `}` to denote the start and end of a for loop, adding a `;` to the ends of lines, etc. You can reference the existing code for examples of how to use the proper syntax.\n",
    "\n",
    "For debugging purposes, you can add print statements to the code. Printing to Energia's Serial Monitor looks like the line below.\n",
    "\n",
    "`Serial.println(\"I'm being printed!\");`\n",
    "\n",
    "There are 3 code blocks (`PCA1/2/3`) that you need to modify. <b>You should not have to change anything else outside these marked code blocks and the pin definition if you're using a different pin than the default `P6.0` pin.</b> \n",
    "\n",
    "**`CODE BLOCK PCA1`**\n",
    "- Copy the `SNIPPET_SIZE`, `PRELENGTH` and `THRESHOLD` from the appendix section of the PCA ipynb.\n",
    "- Read the following to set `EUCLIDEAN_THRESHOLD` and `LOUDNESS_THRESHOLD`.\n",
    "\n",
    "**`EUCLIDEAN_THRESHOLD` filters the classification depending on a sample's distance to the closest centroid.** If the L2 norm (distance) is larger than the threshold, your classification algorithm should simply ignore it and wait for the next sample. Look at the plot of your data clusters and the centroids from earlier in this notebook and approximate a radius around the centroids that capture most of the data. **Try to be conservative - it's better to not classify than to misclassify.**\n",
    "\n",
    "**`LOUDNESS_THRESHOLD` filters the classification depending on the amplitude of the recorded data.** If the recorded data is too soft, we do not want to classify it as it is probably noise. Look at the plots of the audio recordings to determine the loudness of your recordings, then determine an appropriate threshold that isn't low enough to be considered noise while also being below the maximum loudness. If the Launchpad classifies noise, increase this constant. If it misses a lot of speech (i.e. thinks your word is noise), decrease this constant. This variable is used internally in the enveloping function.\n",
    "\n",
    "**`CODE BLOCK PCA2`**\n",
    "- Copy the PCA vectors, projected mean vector, and centroids from the appendix section of the SVD/PCA ipynb.\n",
    "- If you are using 3 principal components, add a new `pca_vec3` array. \n",
    "- Note: Using more principal components increases the dimensionality of the centroids and projections, but also consumes more of the limited memory on the Launchpad.\n",
    "\n",
    "**`CODE BLOCK PCA3`**\n",
    "- This is the actual classification algorithm.\n",
    "- Before this block, the call to `envelope` leaves the data vector of your recording in the array called `result`.\n",
    "- Project this data onto your new PCA basis.\n",
    "    - Use just one loop to project your recorded data vector onto both (or all 3, if you're using 3) of your PCA vectors.\n",
    "    - Use the variables `proj1` and `proj2` to store the projection results.\n",
    "    - If you're using 3 vectors, create a variable `proj3` in the same way `proj1` and `proj2` are defined.\n",
    "- Demean the projection.\n",
    "    - Remember that we **demean after projecting** to save memory on the Launchpad. Instead of finding $x - \\bar{x}$ and then projecting onto the PCA basis, we first project and then demean, using $ y = x_{\\text{proj}} - \\bar{x}_{\\text{proj}} $, where $x_{\\text{proj}}$ is the projection of your data vector onto the PCA basis, and $\\bar{x}_{\\text{proj}}$ is the projection of your mean vector onto the PCA basis.\n",
    "- Classify the projections using the centroids.\n",
    "    - Find the distance between the projected data point and each centroid using the function `l2_norm` (for 2 principal components) or `l2_norm3` (for 3 principal components). Look up the function definition in the sketch.\n",
    "    - Out of the 4 centroids, find the one with the smallest L2 norm.\n",
    "    - Verify this distance is less than `EUCLIDEAN_THRESHOLD`. If it's not, print out an error statement saying this threshold was not satisfied so you have an easier time debugging.\n",
    "- Print the classification to the Serial Monitor. The baud rate for this program is 38400, so make sure you change the baud rate of your Serial Monitor accordingly!\n",
    "\n",
    "Before testing the code, probe the mic board's output with your oscilloscope and make sure that it is still centered around 1.65V. Now upload the sketch, (re)open the Serial Monitor, and press the reset button. Say your word and the Launchpad should recognize it! Try to get a reasonable accuracy (like at least 80-90%); it's okay if it misclassifies occasionally, but it should be accurate in general!\n",
    "\n",
    "**<span style=\"color:red\">If the Launchpad does not classify as well as you think it should, remember to play with the `EUCLIDEAN_THRESHOLD` and `LOUDNESS_THRESHOLD` variables.</span>** To debug the sketch, you can also print out any of the variables you have used, like the distance to the closest centroid.\n",
    "\n",
    "Voila! Your SIXT33N car can recognize your words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Summary TODO</span>**\n",
    "- **<span style=\"color:red\">Fill in `CODE BLOCK PCA1`: Fill out `SNIPPET_SIZE`, `PRELENGTH`, `THRESHOLD`, `EUCLIDEAN_THRESHOLD`, and `LOUDNESS_THRESHOLD`</span>** \n",
    "- **<span style=\"color:red\">Fill in `CODE BLOCK PCA2`: Copy the principal components, projected mean vector, and centroids from the SVD/PCA Jupyter notebook</span>**\n",
    "- **<span style=\"color:red\">Fill in `CODE BLOCK PCA3`: Do the actual classification.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width='30px' align='left' src=\"http://inst.eecs.berkeley.edu/~ee16b/sp16/lab_pics/check.png\">\n",
    "\n",
    "## <span style=\"color:green\">CHECKOFF</span>\n",
    "\n",
    "- **Have all questions, code, and plots completed in this notebook.** Your GSI will check all your PCA code and plots.\n",
    "- **Show your GSI that you've achieved 80% accuracy on your test data for all 4 words.** \n",
    "- Your GSI will also check that you have **submitted your `.csv` files for the 4 words to the Gradescope assignment: \"[Hands-on] SVD/PCA Files.\" Only one person in lab group needs to submit and needs to add all group members to same submission on Gradescope!**\n",
    "- **Show your GSI that you are able to classify live while running `classify.ino`.**\n",
    "- **Be prepared to answer conceptual questions about the lab.** Make sure you have read the lab note before requesting a checkoff! Many checkoff questions are pulled straight from the lab note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkoff Request Link: https://links.eecs16b.org/lab-checkoff-sp22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE ALL YOUR DATA!!\n",
    "\n",
    "- Make sure to save the formatted vectors below and `classify.ino` for Integration/Final Demo!\n",
    "- **Data stored on the lab computers often gets deleted automatically.** Please store it on your personal flash drive or cloud storage like Google Drive, and not on the lab computers! If you used DataHub, it should save through your CalNet ID.\n",
    "- **You will need everything for the final lab report.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='appendix'></a>\n",
    "## <span style=\"color:navy\">Appendix: Formatting Vectors for Energia</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, uncomment the following block if you will use the optimizer parameters and leave it commented if you will use the original parameters. Then run the following code blocks and copy/paste the following printed code into **`classify.ino`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to use the optimizer parameters\n",
    "\"\"\"\n",
    "length = length_opt\n",
    "pre_length = pre_length_opt\n",
    "threshold = threshold_opt\n",
    "new_basis = new_basis_opt\n",
    "projected_mean_vec = projected_mean_vec_opt\n",
    "centroids = centroids_opt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Paste the code below into 'CODE BLOCK PCA1':\")\n",
    "print(\"\")\n",
    "print(utils.format_constant_energia(\"SNIPPET_SIZE\", length))\n",
    "print(utils.format_constant_energia(\"PRELENGTH\", pre_length))\n",
    "print(utils.format_constant_energia(\"THRESHOLD\", threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Paste the code below into 'CODE BLOCK PCA2':\")\n",
    "print(\"\")\n",
    "print(utils.format_array_energia(\"pca_vec1\", new_basis[:, 0]))\n",
    "print(utils.format_array_energia(\"pca_vec2\", new_basis[:, 1]))\n",
    "if new_basis.shape[1] == 3:\n",
    "    print(utils.format_array_energia(\"pca_vec3\", new_basis[:, 2]))\n",
    "print(utils.format_array_energia(\"projected_mean_vec\", projected_mean_vec))\n",
    "print(utils.format_array_energia(\"centroid1\", centroids[0]))\n",
    "print(utils.format_array_energia(\"centroid2\", centroids[1]))\n",
    "print(utils.format_array_energia(\"centroid3\", centroids[2]))\n",
    "print(utils.format_array_energia(\"centroid4\", centroids[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
